{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do's: \n",
    "\n",
    "- zwei code blocks, die mit listen arbeiten in arrays umwandeln -> performanter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Books: \n",
    "\n",
    "https://www.taylorfrancis.com/books/edit/10.1201/9780367631888/recommender-systems-pavan-kumar-vairachilai-sirisha-potluri-sachi-nandan-mohanty\n",
    "\n",
    "https://beluga.sub.uni-hamburg.de/vufind/Record/1656091313?rank=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "from surprise import Reader, Dataset\n",
    "from surprise import KNNBasic, SVD, CoClustering, SlopeOne\n",
    "from surprise.model_selection import KFold, RepeatedKFold, cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"person-skills_2022-06-27.csv\",sep=\";\")\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So later functions work, it makes sense to rename persons from 1 to maxno. \n",
    "# Create dictionary which matches the employee\n",
    "persons = data[\"person\"].unique()\n",
    "#np.arange(1,len(data[\"person\"].unique())+1)\n",
    "skills = sorted(data[\"skill\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_per_person = []\n",
    "for person in data[\"person\"].unique(): \n",
    "    skillset = []\n",
    "    for skill in skills:\n",
    "        if skill in data[data[\"person\"] == person][\"skill\"].unique():\n",
    "            skillset.append(1)\n",
    "        else:\n",
    "            skillset.append(0)\n",
    "    skills_per_person.append(skillset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(index=persons,columns=skills,data=skills_per_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df#[~df[\"category\"].isin([\"Betriebssystem\", \"Dienste\", \"Einsatzfelder / Erfahrungen / Schwerpunkte\", \"Rolle (intern)\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_dict = {}\n",
    "for n in range(1,data[\"person\"].nunique()+1):\n",
    "    employee_dict[n] = data[\"person\"].unique()[n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So later functions work, it makes sense to rename persons from 1 to maxno. \n",
    "# Create dictionary which matches the employee\n",
    "persons = data[\"person\"].unique()\n",
    "#np.arange(1,len(data[\"person\"].unique())+1)\n",
    "skills = sorted(data[\"skill\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_per_person = []\n",
    "for person in data[\"person\"].unique(): \n",
    "    skillset = []\n",
    "    for skill in skills:\n",
    "        if skill in data[data[\"person\"] == person][\"skill\"].unique():\n",
    "            skillset.append(1)\n",
    "        else:\n",
    "            skillset.append(0)\n",
    "    skills_per_person.append(skillset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(index=persons,columns=skills,data=skills_per_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employee-employee approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between employees: due to dichotomous nature of data, Jaccard similarity is used for computing the similarity between employees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_binary(x,y):\n",
    "    \"\"\"A function for finding the similarity between two binary vectors\"\"\"\n",
    "    intersection = np.logical_and(x, y)\n",
    "    union = np.logical_or(x, y)\n",
    "    similarity = intersection.sum() / float(union.sum())\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_sim(data,employee):\n",
    "    sim = pd.DataFrame(index=[employee])\n",
    "    employee_data = data.loc[employee]\n",
    "    sim_data = data.drop(employee,axis=0)\n",
    "    for emp in sim_data.index:\n",
    "        new_sim = pd.DataFrame(index=[employee],columns=[emp],data=jaccard_binary(employee_data,sim_data.loc[emp]))\n",
    "        sim = pd.concat([sim,new_sim],axis=1)\n",
    "    return sim       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_nearest_neighbors(data,employee,n):\n",
    "    neighbors = pd.DataFrame(index=[employee])\n",
    "    similarities = compute_jaccard_sim(data,employee)\n",
    "    for i in range(0,n):\n",
    "        nearest_neighbor = similarities[similarities.idxmax(axis=1)]\n",
    "        neighbors = pd.concat([neighbors,nearest_neighbor],axis=1)\n",
    "        similarities.drop(nearest_neighbor,axis=1,inplace=True)\n",
    "    return neighbors.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_skills_for_employee(data,employee,n_neighbors):\n",
    "    neighbors = compute_n_nearest_neighbors(data,employee,n_neighbors)\n",
    "    values_of_employee = data.loc[employee].to_numpy()\n",
    "    neighbors_matrix = data.loc[neighbors].to_numpy()\n",
    "    predicted_values = np.array([])\n",
    "    for i in range(0,len(values_of_employee)):\n",
    "        if values_of_employee[i] == 0: \n",
    "            predicted_values = np.append(predicted_values,\n",
    "                                            # weighted averahe might be more accurate\n",
    "                                            np.mean(neighbors_matrix[:,i]))\n",
    "        else: \n",
    "            predicted_values = np.append(predicted_values,values_of_employee[i])\n",
    "    return predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_skills_for_matrix(data,n_neighbors): \n",
    "    # create emppy array of length according to columns (in order for later vstack to work)\n",
    "    return_data = np.zeros(len(data.columns))\n",
    "    for i in data.index: \n",
    "        single_predicition = predict_skills_for_employee(data,i,n_neighbors)\n",
    "        return_data = np.vstack([return_data,single_predicition])\n",
    "    # delete first entry (zeros) from return data\n",
    "    return_data = return_data[1:len(return_data)]\n",
    "    return pd.DataFrame(index=data.index,columns=data.columns,data=return_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_skills_for_matrix(matrix,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Facorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create matrix with values between 1 and 5 for recommender trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values between 1 and 5 should not be arbitrary. As weight for their score, the frequency of the underlying category for each skill will be taken into account. For instance, if employee 12 has four skills in the category programming language and one skill in data banks, each programming skill will be evaluated with a respectivley high score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_to_category = pd.read_csv(\"mitarbeiterportal-skills_2022-06-21.csv\",sep=\";\",header=None)\n",
    "category_dict = dict(zip(skills_to_category[0],skills_to_category[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_1_5(x,old_max,old_min):\n",
    "    OldRange = (old_max - old_min)  \n",
    "    if (OldRange == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        NewRange = (5 - 1)  \n",
    "        return round((((x - 1) * NewRange) / OldRange) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for each employee and their category count\n",
    "categories = data[\"category\"].unique()\n",
    "category_per_employee = pd.DataFrame(index=categories)\n",
    "for p in persons:\n",
    "    p_skills = data[data[\"person\"] == p]\n",
    "    # count categories for each employee\n",
    "    category_count = p_skills[\"category\"].value_counts()\n",
    "    category_count = category_count.apply(lambda x:scaler_1_5(x,category_count.max(),category_count.min()))\n",
    "    skill_values = np.array([])\n",
    "    # add count for each category to an array and 0 if category wasn't found (and therefore is not part of resptive employee's skillset)\n",
    "    for cat in categories:\n",
    "        try:\n",
    "            skill_values = np.append(skill_values,category_count[cat])\n",
    "        except KeyError:\n",
    "            skill_values = np.append(skill_values,0)\n",
    "    # add employee data to dataframe\n",
    "    p_skills_df = pd.DataFrame(index=categories,columns=[p],data=skill_values)\n",
    "    category_per_employee = pd.concat([category_per_employee,p_skills_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_per_person_weighted = []\n",
    "for p in persons: \n",
    "    skillset = []\n",
    "    for skill in skills:\n",
    "        if skill in data[data[\"person\"] == p][\"skill\"].unique():\n",
    "            skillset.append(category_per_employee[p][category_dict[skill]])\n",
    "        else:\n",
    "            skillset.append(0)    \n",
    "    skills_per_person_weighted.append(skillset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_mod = pd.DataFrame(index=persons,columns=skills,data=skills_per_person_weighted)\n",
    "matrix_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset, SVD, accuracy\n",
    "#from surprise.model_selection import cross_validate, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_mod[matrix_mod == 0].count().sum()/matrix_mod[matrix_mod != 0].count().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 times more 0 values than values with a rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_alphabetically = sorted(skills_to_category[0].to_list())\n",
    "skills_dict = {x:skill_alphabetically[x] for x in range(0,len(skill_alphabetically))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_dict_inv = {v: k for k, v in employee_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data structure suitable for surprise library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np = df.to_numpy()\n",
    "\n",
    "for i in range(0,len(df_np)):\n",
    "    df_np[i][2] = matrix_mod.loc[df_np[i][0]][df_np[i][1]]\n",
    "\n",
    "df_rated = pd.DataFrame(df_np)\n",
    "df_rated.columns = [\"Employee\",\"Skill\",\"Rating\"]\n",
    "df_rated    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_rating = pd.concat([df_rated,df[\"category\"]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader()\n",
    "svd_data = Dataset.load_from_df(df_rated,reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE >= 0.75 -> bueno <br>\n",
    "MAE? no sé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = SVD()\n",
    "cv = cross_validate(svd, svd_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(svd_data, test_size=0.25)\n",
    "svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svd.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills_of_employee(emp_id):\n",
    "    df_category_rating[df_category_rating[\"Employee\"] == emp_id][\"category\"].value_counts().plot(kind=\"bar\",ylabel=\"Count\",title=\"Skill Category Portfolio Employee \"+str(emp_id))\n",
    "    return df_category_rating[df_category_rating[\"Employee\"] == emp_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual prediciton testing: input skill of employee that they already know and check the prediciton vs. the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_id = str(1479)\n",
    "skill_id = \".NET Core\" \n",
    "\n",
    "# get a prediction for specific users and items.\n",
    "pred = svd.predict(employee_id, skill_id, r_ui=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_skills_of_employee(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOW TO EVALUATE RECOMMENATIONS? <BR>\n",
    "HOW TO IMPLEMENT NEAT WAY OF GETTING RECOMMENDATIONS/RECOMMEND EMPLOYEES FOR CERTAIN SKILLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create item-based representation of DataFrame (cause we might need it later?)\n",
    "df_item_based = df_category_rating.sort_values(by=\"Skill\")[[\"Skill\",\"Employee\",\"Rating\",\"category\"]]\n",
    "item_based_data = df_item_based[[\"Skill\",\"Employee\",\"Rating\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Different Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets pick some algorithms to include into our ensemble. We'll choose four.\n",
    "-  Collaborative Filtering\n",
    "-  Matrix Factorization\n",
    "-  collaborative filtering with co-clustering\n",
    "-  Collaborative Filtering based on the popular Slope One Algorithm\n",
    "\n",
    "https://www.kaggle.com/code/robottums/hybrid-recommender-systems-with-surprise/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset\n",
    "from surprise import KNNBasic, KNNWithMeans, SVD, CoClustering, SlopeOne, SVDpp, NMF, BaselineOnly\n",
    "from surprise.model_selection import KFold, RepeatedKFold, cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Try to recreate cross validation set up from above link and compare different recommendation techiques. \n",
    "Ablauf: run cros val and protocol KPI's (RMSE etc.), then build comprehensive recommendation function for best performing recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1,5))\n",
    "svd_data = Dataset.load_from_df(df_rated,reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 5\n",
    "NUM_OUTER_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv(algo,data):\n",
    "    \n",
    "    start = time.time()\n",
    "    rmse = np.zeros((NUM_TRIALS, NUM_TRIALS))\n",
    "    mae = np.zeros((NUM_TRIALS, NUM_TRIALS))\n",
    "    fit_times = np.zeros((NUM_TRIALS, NUM_TRIALS))\n",
    "    test_times = np.zeros((NUM_TRIALS, NUM_TRIALS))\n",
    "\n",
    "    for i in range(NUM_TRIALS):\n",
    "        outer_cv = KFold(n_splits=NUM_OUTER_SPLITS,shuffle=True,random_state=36)\n",
    "        cv_results = cross_validate(algo=algo,data=data,measures=[\"rmse\",\"mae\"],cv=outer_cv,n_jobs=8)\n",
    "        rmse[i] = cv_results[\"test_rmse\"]\n",
    "        mae[i] = cv_results[\"test_mae\"]\n",
    "        fit_times[i] = cv_results[\"fit_time\"]\n",
    "        test_times[i] = cv_results[\"test_time\"]\n",
    "\n",
    "    return rmse,mae, fit_times, test_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_result(results, name, accs, baccs, fit_times, test_times):\n",
    "    '''\n",
    "    Function adding the results returned by nested_cv to a dataframe. \n",
    "    Results will be aggregated for better comparison. \n",
    "    Parameters: \n",
    "    results = DataFrame which the results should be added to\n",
    "    name = string describing the estimator which values are to be added\n",
    "    accs = accuracy values of estimator\n",
    "    baccs = balanced accuracy values of estimator \n",
    "    fit_times = fitting times of estimator\n",
    "    test_times = testing times of estimator\n",
    "    Output:\n",
    "    results DataFrame containing an additional row\n",
    "    '''\n",
    "    row = pd.DataFrame({\n",
    "        \"name\":name,\n",
    "        \"rmse_mean\":accs.mean(), \n",
    "        \"rmse_std\":accs.std(), \n",
    "        \"rmse_min\":accs.min(), \n",
    "        \"rmse_max\":accs.max(), \n",
    "        \"mae_mean\":baccs.mean(), \n",
    "        \"mae_std\":baccs.std(), \n",
    "        \"mae_min\":baccs.min(), \n",
    "        \"mae_max\":baccs.max(), \n",
    "        \"fit_time\":fit_times.mean(),\n",
    "        \"test_time\":test_times.mean()\n",
    "        },index=[0])\n",
    "    return pd.concat([results,row],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list = [(BaselineOnly(),\"Baseline\"),\n",
    "                (KNNBasic(),\"k-NN\"),\n",
    "                (KNNWithMeans(),\"Centered k-NN\"),\n",
    "                (SVD(),\"SVD\"),\n",
    "                (SVDpp(),\"SVD++\"),\n",
    "                (CoClustering(),\"CoClustering\"),\n",
    "                (SlopeOne(),\"SlopeOne\"),\n",
    "                (NMF(),\"NMF\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo in algo_list:\n",
    "    rmse, mae, fit_times, test_times = nested_cv(algo[0],svd_data)\n",
    "    results = add_result(results,algo[1],rmse,mae,fit_times,test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(algo,grid,data):\n",
    "    params = {}\n",
    "    for i in range(NUM_TRIALS):\n",
    "        cv = RepeatedKFold(n_splits=5)\n",
    "        model = GridSearchCV(algo_class=algo,param_grid=grid,cv=cv,measures=[\"rmse\", \"mae\"],refit=\"rmse\")\n",
    "        model.fit(data)\n",
    "        params = model.best_params[\"rmse\"]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo = SVDpp\n",
    "# grid = {\"n_factors\":[10,50,100],\n",
    "#         \"n_epochs\":[10,20,40]\n",
    "#         #,\"init_mean\":[0,0.5,1],\n",
    "#         # \"init_std_dev\":[0.1,0.3]\n",
    "#         }\n",
    "# params = get_best_params(algo,grid,svd_data)\n",
    "\n",
    "# algo = SVDpp(n_factors=params[\"n_factors\"],n_epochs=params[\"n_epochs\"])#,biased=params[\"biased\"])\n",
    "\n",
    "# rmse, mae, fit_times, test_times = nested_cv(algo,svd_data)\n",
    "# results = add_result(results,\"SVD++_best_params\",rmse,mae,fit_times,test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo = NMF\n",
    "# grid = {\"n_factors\":[10,50,100],\"n_epochs\":[20,40,80],\"biased\":[True,False]}\n",
    "# params = get_best_params(algo,grid,svd_data)\n",
    "\n",
    "# algo = NMF(n_factors=params[\"n_factors\"],n_epochs=params[\"n_epochs\"],biased=params[\"biased\"])\n",
    "\n",
    "# rmse, mae, fit_times, test_times = nested_cv(algo,svd_data)\n",
    "# results = add_result(results,\"NMF_best_params\",rmse,mae,fit_times,test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do's : \n",
    "-  Explore sim_options of some recommenders\n",
    "-  Think of useful param grid options\n",
    "-  Make KNNBasic shut the f up \n",
    "-  Choose 1 algorihm\n",
    " -  Think of evaluation method \n",
    " -  Work with actual recoms (see which skills get recommended) and see if it's sensual \n",
    "\n",
    "-  Binary problem? Can we apply this stuff to it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next steps: \n",
    "modell definieren according to best algo with best params, die funktion oben ausprobieren mit predict und dann die top_n ausgeben lassen, eventueller vergleich zur baseline funktion, funktion schreiben, die NEUEN user sachen vorschlägt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('ds_ap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52cefc3a9eff3ff32f2531cecf7f574c5eb7023f21571ad3a673d9838121c504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
